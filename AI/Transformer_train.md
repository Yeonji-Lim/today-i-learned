# Transformer의 학습

트랜스포머의 학습(train)은 인코더와 디코더 입력이 주어졌을 때 

모델 최종 출력에서 정답에 해당하는 단어의 확률 값을 높이는 방식으로 수행됨

자세한 과정을 알기 위해서는 셀프 어텐션의 내부 동작을 알아야 함

# Self Attention

단어 임베딩 차원수( 𝑑 )가 4이고, 인코더에 입력된 단어 갯수가 3일 경우

셀프 어텐션의 입력 형식은 다음과 같다고 하자.

~~~
x = [[1, 0, 1, 0],
     [0, 2, 0, 2],
     [1, 1, 1, 1]]
~~~

셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 3개 요소 사이의 문맥적 관계성을 추출하는 과정

입력 벡터 시퀀스( 𝐗 )에 쿼리, 키, 밸류를 만들어주는 행렬( 𝐖 )을 각각 곱합니다. 

~~~
Q = X * W_Q
K = X * W_K
V = X * W_V
~~~

W 들은 가중치로, 학습할 때 더 정답에 부합하는 방향으로 업데이트 되게 됨

~~~
Attention(Q, K, V) = (softmax( (Q*K_T)/sqrt(d_k) ))*V
~~~

어텐션 함수는 위와 같은 수식으로, Q와 K의 유사도를 측정하고 나온 값을 V에 가중합하여 반영함

이 V는 각 토큰의 Attention Value로, 해당 토큰이 현재 시퀀스(K)에서의 유사도를 측정.