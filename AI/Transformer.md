# Transformer
OXAI 제공 링크 : http://jalammar.github.io/illustrated-transformer/
(그런데 한국어 문서로 먼저 이해함)

seq2seq모델의 한계 -> Attention Mechanism -> Transformer -> stage2 of Deep Saber

## seq2seq모델
https://wikidocs.net/24996

어떤 시퀀스 데이터를 다른 도메인에 있는 시퀀스 데이터로 변환하고자 할 때 쓰는 모델

보통 챗봇이나 기계 번역에 쓰임


두개의 모듈로 분리되고 각각 RNN 셀들로 구성되어 있음 -> 두 개의 RNN 아키텍처라고 볼 수 있다.

- 인코더 : 입력한 시퀀스의 데이터를 순차적으로 입력받고 모든 데이터 정보들을 압축해서 하나의 Context Vector를 만들어 디코더에게 전달

- 디코더 : 테스트 과정과 훈련과정이 다름

테스트 과정 - 컨텍스트 벡터와 시작을 의미하는 심볼 <sos>를 받아서 변환된 데이터를 순차적으로 출력, 다음에 올 데이터를 예측하고, 예측한 데이터를 다음 시점의 RNN 셀로 입력하는 것을 끝을 의미하는 심볼 <eos>가 예측될 때까지 반복

훈련 과정 - 컨텍스트 벡터와 실제 정답, 예를 들어 <sos> a b c 를 입력받고, 정답으로 a b c <eos>가 나와야한다고 정답을 알려주면서 훈련


시퀀스의 각 데이터를 벡터로 바꿀때 임베딩을 사용한다. 각각의 임베딩 벡터가 있는 것


인코더의 RNN 셀 하나을 볼 때 그 시점이 t라고 하면,

t-1에서의 은닉상태와 t에서의 임베딩 벡터를 입력 받고

t에서의 은닉상태를 출력함


이때 은닉 상태 -> 과거 시점의 동일한 RNN 셀에서의 모든 은닉상태의 값들의 영향을 누적해서 받은 것

인코더의 마지막 셀의 은닉 상태 = 컨텍스트 벡터 = 입력 시퀀스 내의 데이터들에 대한 정보 요약

...


## Attention Mechanism

https://wikidocs.net/22893

신경망들의 성능을 높이기 위한 메커니즘.


seq2seq의 한계

1. 하나의 벡터로 압축 -> 정보의 손실이 발생

2. RNN의 문제 -> Vanishing Gradient


입력 시퀀스가 길어질 수록 품질이 떨어진다. 정확도 보정의 기법이 어텐션


디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서의 전체 입력 문장을 다시 참고하는데,

해당 시점에서 예측해야할 데이터와 연관이 있는 입력 데이터 부분을 더 집중해서 봄


어텐션 함수는 다음과 같이 간단히 표현할 수 있음

~~~
Attention(Q, K, V) = Attention Value

seq2seq + Attention 모델에서 의미
Q = Query : t 시점의 디코더 셀에서의 은닉 상태
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
~~~

어떤 Query에 대해서 모든 Key와의 유사도를 구함 (Soft max)

이 유사도를 Key에 해당하는 Value에 반영

Value들을 모두 더해서 리턴 -> Attention Value

...


## Transformer


seq2seq의 단점을 개선하면서도 인코더-디코더 구조를 유지하는 모델

Attention 만으로 구현한 모델, RNN을 사용하지 않음..! (에?)

RNN을 보정하는 데에 Attention을 쓰는게 아니라 그냥 어텐션만 쓰겠다는 것임

___

### 트랜스포머의 하이퍼파라미터

✽ 하이퍼파라미터 : 사용자가 모델 설계시 임의로 변경할 수 있는 값

___
  
~~~
d_model = 512
~~~

트랜스 포머의 인코더와 디코더에서 정해진 입력과 출력의 크기,

임베딩 벡터의 차원,

인코더와 디코더가 다음층의 인코더와 디코더로 값을 보낼 때의 차원,

트랜스포머 내부의 피드 포워드 신경망의 입력층과 출력층의 크기

논문에서는 512

___
  
~~~
num_layers = 6
~~~

트랜스포머에서 하나의 인코더와 디코더를 층으로 생각하였을 때, 총 층수

논문에서 6

___
  
~~~
num_heads = 8
~~~

트랜스포머에서 어텐션을 사용할 때 한번 하는 것보다

여러 개로 분할해서 병렬로 어텐션을 수행하고

결과값을 하나로 합침

이때 병렬의 개수가 8이라는 것임

___
  
~~~
d_ff = 2048
~~~

트랜스포머 내부의 피드 포워드 신경망에서 은닉층의 크기

___

### 큰 구조는 동일하다.

기존의 seq2seq는 각각 하나의 인코더와 디코더 안에 시점의 개수만큼의 RNN 셀을 가지는 구조였다면

트랜스포머는 인코더와 디코더 자체가 각각 N개로 구성되어 있다.

그리고 내부 구조는 다르지만 전체적으로 보았을 때 입력과 출력의 형식, 그리고 인코더-디코더 구조를 갖는 것이 seq2seq와 동일하다.

내부 구조를 자세하게 이해하기 전에 Positional Encoding을 알아야 함

___

### Positional Encoding

트랜스포머의 인코더 디코더는 임베딩 벡터를 바로 입력 받지 않고 그 벡터에서 포지셔널 인코딩으로 조정된 값을 입력으로 받음

RNN은 데이터의 위치에 따라 순차적으로 입력받아서 처리하는 특성으로 각 데이터의 위치정보를 가질 수 있었다

그런데 트랜스포머에서는 RNN을 쓰지 않기 때문에 포지셔널 인코딩으로 각 데이터의 임베딩 벡터에 위치정보를 더해줌

이를 통해 순서 정보가 보존되는데 같은 데이터로 구성되었다고 하더라도 위치에 따라서 입력값이 달라지는 것

___

### Attention

트랜스포머에서는 세가지 어텐션이 사용됨

1. Encoder Self-Attention -> Query = Key = Value

2. Masked Decoder Self-Attention -> Query = Key = Value

3. Encoder-Decoder Attention -> Query : Decoder's Vector / Key = Value : Encoder's Vector


Self Attention은 Query, Key, Value 모두 그 벡터의 출처가 같다


## Encoder


트랜스 포머는 여러개의 인코더로 마치 층처럼 쌓여있는데

하나의 인코더는 또 그 안에 2개의 서브 층이 있다.

셀프 어텐션과 피드 포워드 신경망(FFNN)이다.


## Self Attention

어텐션 함수는 주어진 쿼리에 대해서 모든 키와의 유사도를 구하고

이 유사도를 가중치로 각 키의 value에 반영해준다. 그리고 이 값들을 모두 가중합 하여 리턴한 것이 어텐션 값


seq2seq를 사용하는 경우 다음과 같다.

~~~
Q = Query : t 시점의 디코더 셀에서의 은닉 상태 -> t가 반복되기 때문에 모든 시점으로 일반화할 수 있다. 
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
~~~

원래는 이와 같이 Q와 K가 디코더셀과 인코더셀이라는 차이점을 가지기 때문에 서로 다른 값을 가지고 있다.

그런데 셀프 어텐션에서는 이 3가지가 모두 동일하다.

~~~
Q : 입력 문장의 모든 단어 벡터들
K : 입력 문장의 모든 단어 벡터들
V : 입력 문장의 모든 단어 벡터들
~~~

이런 셀프 어텐션을 통해 문장의 예시에서 대명사가 어떤 명사를 지칭했던것인지 구분할 수 있는 효과를 가진다.


그런데 이때 단어 벡터들은 초기 입력인 dmodel 의 차원을 가지는 것을 그대로 쓰지 않고 나눠서 Q, K, V벡터를 얻는다.

논문에서는 dmodel =512의 차원을 가졌던 각 단어 벡터들을 64의 차원을 가지는 Q,K,V 벡터로 변환하였다.


64는 num_heads의 값으로 인해 결정된다. 논문에서 num_heads는 그 값이 8이다.

기존 벡터로 부터 각각 해당하는 가중치 행렬(W, W, W)를 곱해서 각 Q, K, V라는 벡터를 얻는데 이 가중치 행렬의 크기가 dmodel x ( dmodel / num_heads )이므로 각 벡터의 크기가 dmodel / num_heads = 64 가 되는 것

이 이후에는 기존의 어텐션 메커니즘과 동일하다

## Position-wise FFNN

인코더와 디코더에서 공통적으로 가지고 있는 서브층, Fully-connected FFNN

✽ FFNN은 RNN과 다르게 입력층에서 출력층 방향으로 연산이 진행되는 신경망을 말한다.

## Add & Norm

서브층 다음에 추가적으로 적용하는 기법,

Add : 잔차 연결 Residual Connection

Norm : 층 정규화 Layer Normalization

___
  
### Residual Connection

잔차 연결은 그 층의 입력과 출력을 더하는 것을 말함

~~~
x + Sublayer(x)
~~~

___
  
### Layer Normalization

~~~
LN = LayerNorm( x + Sublayer(x) )
~~~

텐서의 마지막 차원에 대해서 평균과 분산을 구하고,

이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움

## 디코더

인코더는 층별로 연산을 순차적으로 진행하고 마지막 층에서 그 출력을 디코더에게 전달.

디코더에서는 그 안에서 인코더가 보낸 출력을 다시 이용하여 연산한다.


디코더는 3개의 서브층으로 구성되어 있다.

1. 셀프 어텐션과 룩-어헤드 마스크

2. 인코더-디코더 어텐션

3. FFNN


## 셀프 어텐션과 룩-어헤드 마스크


트랜스포머에서도 seq2seq 모델에서와 마찬가지로 Teacher Forcing을 사용하여 훈련된다.

그렇기 때문에 입력은 시퀀스를 한번에 받고, 디코더는 이 시퀀스에서 각 시점의 데이터를 예측하도록 훈련

그런데 seq2seq에서는 RNN을 사용해 현재 시점과 그 이전 시점을 참고하는데,

트랜스포머 같은 경우 그 시점 뒤의 미래시점까지 참고할 수 있음


이를 방지하기 위해 마스크가 필요 -> 미리보기 마스크

인코더의 셀프 어텐션과 동일한 과정을 수행하고 결과인 스코어 행렬에 마스킹 -> 자기 자신과 그 이전의 단어들만 참고할 수 있게 됨


다른 어텐션 처럼 패딩 마스크를 전달할 필요가 잇음 그래서 룩-어헤드 마스크가 패딩 마스크를 포함하도록 구현한다.


## 인코더-디코더 어텐션


이번에는 셀프 어텐션은 아님

셀프 어텐션은 Q, K, V의 출처가 같지만

이 경우 Q는 디코더, K, V는 인코더의 행렬임

그 외 수행 과정은 다른 어텐션과 같다.

___

정리 하자면,

~~~
인코더의 첫번째 서브층 | 셀프 어텐션          | 패딩 마스크를 전달                    | Query = Key = Value

디코더의 첫번째 서브층 | 마스크드 셀프 어텐션   | 룩-어헤드 마스크(패딩 마스크 포함)를 전달  | Query = Key = Value

디코더의 두번째 서브층 | 인코더-디코더 어텐션   | 패딩 마스크를 전달                    | Query : 디코더 행렬 / Key = Value : 인코더 행렬
~~~
